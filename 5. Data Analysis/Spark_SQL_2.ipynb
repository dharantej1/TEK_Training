{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97a1af1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x1e2d07589d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sqlite3\n",
    "con=sqlite3.connect('product2.db')\n",
    "cur=con.cursor()\n",
    "\n",
    "\n",
    "cur.execute('''CREATE TABLE IF NOT EXISTS products (date text,trans text,product_id int, product_name text,qty real,price float)''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fd97b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import findspark\n",
    "findspark.init()\n",
    "import os\n",
    "spark=SparkSession.builder.appName('SparkSQL_UseCase').master('local[2]').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "659d304a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x1e2d07589d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur.execute(\"INSERT INTO products VALUES('2023-04-05','credit card',1331,'Iphone',10,35.14)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01dad5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "con.commit()\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "726ce734",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder \\\n",
    ".master(\"local\") \\\n",
    ".appName(\"SQLite JDBC\") \\\n",
    ".config(\"spark.jars\",\"C:\\\\Users\\\\pdharantej\\\\OneDrive - ALLEGIS GROUP\\\\Desktop\\\\TEK_Training\\\\5. Data Analysis\\\\db_&_jarfiles_spark_sql\\\\sqlite-jdbc-3.34.0.jar\")\\\n",
    ".config(\"spark.driver.extraClassPath\",\"C:\\\\Users\\\\pdharantej\\\\OneDrive - ALLEGIS GROUP\\\\Desktop\\\\TEK_Training\\\\5. Data Analysis\\\\db_&_jarfiles_spark_sql\\\\sqlite-jdbc-3.34.0.jar\") \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07f1a09e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\pdharantej\\\\OneDrive - ALLEGIS GROUP\\\\Desktop\\\\TEK_Training\\\\5. Data Analysis'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37b857df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# products_df=spark.read.format(\"jdbc\").option(\"url\",\"jdbc:sqlite://{}/product2.db\".format(os.getcwd())).option(\"driver\",\"org.sqlite.JDBC\").option(\"dbtable\",\"products\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5b3df868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# userDetails = {'userName': 'root', 'pwd':'ThinkPad@66'}\n",
    "# import pyspark\n",
    "# from pyspark.sql import SparkSession\n",
    "# spark = SparkSession.builder.config(\"spark.jars\", \"C:\\\\Users\\\\pdharantej\\\\Downloads\\\\mysql-connector-java-8.0.32.jar\") \\\n",
    "#     .master(\"local[*]\").appName(\"PySpark_MySQL_test\").getOrCreate()\n",
    "# products_df = spark.read.format(\"jdbc\").option(\"url\", \"jdbc:mysql://localhost:3306/world\") \\\n",
    "#     .option(\"driver\", \"com.mysql.jdbc.Driver\").option(\"dbtable\", \"country\") \\\n",
    "#     .option(\"user\", userDetails['userName']).option(\"password\", userDetails['pwd']).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90083dab",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'products_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18088\\380680366.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mproducts_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'products_df' is not defined"
     ]
    }
   ],
   "source": [
    "products_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15baa62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "custDF=spark.read.format('csv').option('header','true').option('inferSchema','true').load('./Customers.csvs')\n",
    "salesDF=spark.read.format('csv').option('header','true').option('inferSchema','true').load('./Sales.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d75a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "custDF=custDF.withColumnRenamed('ID','customer_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e691f9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "custDF.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17613923",
   "metadata": {},
   "outputs": [],
   "source": [
    "salesDF=salesDF.withColumnRenamed('Customer ID','customer_id')\n",
    "salesDF.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d231ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "broadcastJoin=custDF.hint(\"broadcast\").join(salesDF,\"customer_id\")\n",
    "broadcastJoin.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8e2aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffleHashJoin = custDF.hint('shuffle_hash').join(salesDF, 'customer_id')\n",
    "shuffleHashJoin.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcac140",
   "metadata": {},
   "outputs": [],
   "source": [
    "cartesianProduct = custDF.hint('shuffle_replicate_nl').join(salesDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b798bca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cartesianProduct.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d654ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = custDF.groupBy('company').count()\n",
    "df1.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b7c446",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.adaptive.enabled', True)\n",
    "spark.conf.set('spark.sql.adaptive.coalescePartitions.enabled', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dbfb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = custDF.groupBy('company').count()\n",
    "df1.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8754bd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Higher order functions\n",
    "\n",
    "from pyspark.sql.functions import transform, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69012dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|     doubled|\n",
      "+------------+\n",
      "|[2, 4, 6, 8]|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.createDataFrame([(1, [1,2,3,4])], ('key', 'values'))\n",
    "df.select(transform('values', lambda x: x*2).alias('doubled')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb919837",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alternate(x, i):\n",
    "    return when(i%2==0, x).otherwise(-x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6864d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|    alternated|\n",
      "+--------------+\n",
      "|[1, -2, 3, -4]|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(transform('values', alternate).alias('alternated')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc26a2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|orig_array|      new|\n",
      "+----------+---------+\n",
      "| [1, 2, 3]|[2, 3, 4]|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select array(1,2,3) as orig_array, transform(array(1,2,3), x->x+1) as new').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e7cc94c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- col1: long (nullable = true)\n",
      " |-- col2: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      "\n",
      "+----+------------+\n",
      "|col1|        col2|\n",
      "+----+------------+\n",
      "|   3|   [1, 2, 3]|\n",
      "|   4|[5, 6, 7, 8]|\n",
      "+----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "arrayData=[(3,[1,2,3]),(4,[5,6,7,8])]\n",
    "df=spark.createDataFrame(data=arrayData ,schema=['col1','col2'])\n",
    "df.printSchema()\n",
    "df.createOrReplaceTempView(\"mytesttable\")\n",
    "spark.sql(\"select * from mytesttable\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "19e5aada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+---------+\n",
      "|col1|origin_array|new_array|\n",
      "+----+------------+---------+\n",
      "|3   |[1, 2, 3]   |[8]      |\n",
      "|4   |[5, 6, 7, 8]|[12]     |\n",
      "+----+------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select col1, col2 as origin_array, transform(filter(col2, (x, i)->i=2), x->x+5) as new_array from mytesttable').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f573cd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+---------------+\n",
      "|col1|origin_array|new_array      |\n",
      "+----+------------+---------------+\n",
      "|3   |[1, 2, 3]   |[4, 5, 6]      |\n",
      "|4   |[5, 6, 7, 8]|[9, 10, 11, 12]|\n",
      "+----+------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select col1, col2 as origin_array, transform(col2, (x, i)->x+col1) as new_array from mytesttable').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "652dfe32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c3adaf96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|any_negative|\n",
      "+------------+\n",
      "|       false|\n",
      "|        true|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.createDataFrame([(1, [1,2,3,4]), (2, [3,-1,0])], ('key', 'values'))\n",
    "df.select(exists('values', lambda x: x<0).alias('any_negative')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "02f5a472",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import forall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "16451b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|all_foo|\n",
      "+-------+\n",
      "|  false|\n",
      "|  false|\n",
      "|   true|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.createDataFrame(\n",
    "    [(1, ['bar']), (2,['foo', 'bar']), (3, ['foobar', 'foo'])],\n",
    "    ('key', 'values')\n",
    ")\n",
    "df.select(forall('values', lambda x: x.rlike('foo')).alias('all_foo')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77048587",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
